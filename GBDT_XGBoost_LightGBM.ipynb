{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I am going to introduce a wisely used Machine Learning algorithm in data science area in the industry, Gradient Boosting Decision Tree(GBDT) and its improved versions XGBoost and LightGBM. The brief introduction of GBDT will be given first, and how it is applied to a real dataset for prediction and feature analysis will be demonstrated subsequently. Lately, how XGBoost improves GBDT and how LightGBM improves XGBoost will be shown. Finally, the results of applying these three algorithms on the same dataset will be compared in terms of execution time and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Content\n",
    "\n",
    "After following this tutorial, you should know:\n",
    "        - how GBDT works.\n",
    "        - how to apply GBDT to a real task and do feature analysis and selection.\n",
    "        - how xgboost improves GBDT.\n",
    "        - how lightGBM improves xgboost.\n",
    "\n",
    "- [GBDT](#GBDT)\n",
    "- [Tasks with GBDT](#Tasks-with-GBDT)\n",
    "- [XGBoost](#XGBoost)\n",
    "- [LightGBM](#LightGBM)\n",
    "- [Results Comparison and Conclusion](#Results-Comparison-Conclusion)\n",
    "- [Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT\n",
    "Gradient boosting is a generic technique that generates a powerful model by using an ensemble of multiple arbitrary week learners. Combining this technique with regression tree, GBDT [1] (also called GBRT, GBM) creates powerful prediction models for classification and regression. It trains a series of small trees and each tree is trained to correct the mistakes of the previous trees in the series. The tree is generated by choosing the best split points based on purity scores like Gini, while the model is constructed in an additive manner, that one tree is added at a time and the previous trees remain unchanged. The final prediction of the model is a weighted average prediction of these weak regression trees. \n",
    "\n",
    "The following is a popular example of explaining how  GBDT works. Given features like age, gender, occupation, and if user computer daily, etc., we can predict if someone will like games. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/cart.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Initially, every sample is in the root node. When splitting a node, each value of each feature will be examined based on purity scores like Gini. The value that generates maximal scores will be chosen as the split point. \n",
    "\n",
    "As the above figure shows, age 15 is the most apparent feature that can divide people like computer games and do not like computer games, so it is chosen as the split point for the root node. Whether one node will be split is based on some criteria like minimum number of samples in the node, maximum information gain or minimum impurity decrease for splitting the node, maximum number of leaf nodes, etc. These criteria depend on implementation. Most of them are implemented in open source library, like sklearn. So they can be tuned as hyperparameters. And how deep the tree (depth) can grow is also an import hyperparameter to tune.  \n",
    "\n",
    "Come back to the example, with certain constraints, people are classified into different leaves with a score assigned to each leaf.  \n",
    "\n",
    "All above constraints will influence the growth of the tree and help to deal with overfitting. As you might know, the most significant problem with single decision tree is overfitting. A tree could grow very deeply to fit the data as a small tree perform badly. When the tree becomes too big, the generalization becomes a problem. However, trees have the nice property of interpretation and work well for general purpose tasks. The ensemble idea brings decision tree come to life. Instead of having a deep single tree, the ensemble model has multiple small trees.  \n",
    "\n",
    "The following figure shows how the ensemble of two trees makes the prediction by adding the prediction of each small tree.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/twocart.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "The number of trees in the model is also an important hyperparameter to play with. It is similar with the hyperparameter epochs in the neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks with GBDT\n",
    "\n",
    "As discussed above, GBDT algorithms might be different in terms of implementation. Here I use the GBDT algorithm from scikit-learn. The algorithm is implemented as [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html). They are used for classification task and regression task respectively. \n",
    "\n",
    "\n",
    "The dataset I am going to use to test the algorithm is commonly used dataset [MNIST handwritten digits](https://en.wikipedia.org/wiki/MNIST_database). You can download the data from this [link](http://deeplearning.net/data/mnist/mnist.pkl.gz). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784) (5000,) (1000, 784) (1000,)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    try:\n",
    "        #Python3\n",
    "        train_set, valid_set, test_set = pkl.load(f, encoding='latin1')\n",
    "    except:\n",
    "        #Python2\n",
    "        train_set, valid_set, test_set = pkl.load(f)\n",
    "    f.close()\n",
    "    return(train_set,valid_set,test_set)\n",
    "\n",
    "path = 'mnist.pkl.gz' \n",
    "train_set,valid_set,test_set = load_data(path)\n",
    "\n",
    "# due to computation resource limitation, sample 10% data for this tutorial\n",
    "Xtrain,_,ytrain,_ = train_test_split(train_set[0], train_set[1], test_size=0.9)\n",
    "Xtest,_,ytest,_ = train_test_split(test_set[0], test_set[1], test_size=0.9)\n",
    "print(Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this dataset is for classification task, I use GradientBoostingClassifier. Before applying an algorithm, we have to know about the hyperparameters and how they affect the algorithm. Here is a list of hyperparameters for this classifier:\n",
    "\n",
    "- **learning_rate**: The learning parameter controls the magnitude of this change in the estimates. (default=0.1)\n",
    "- **n_extimators**: The number of sequential trees to be modeled. (default=100)\n",
    "- **max_depth**: The maximum depth of a tree. (default=3)\n",
    "- **min_samples_split**: Tthe minimum number of samples (or observations) which are required in a node to be considered for splitting. (default=2)\n",
    "- **min_samples_leaf**: The minimum samples (or observations) required in a terminal node or leaf. (default=1)\n",
    "- **min_weight_fraction_leaf**: Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer. (default=0.)\n",
    "- **subsample**: The fraction of observations to be selected for each tree. Selection is done by random sampling. (default=1.0)\n",
    "- **max_features**: The number of features to consider while searching for a best split. These will be randomly selected. (default=None)\n",
    "- **max_leaf_nodes**: The maximum number of terminal nodes or leaves in a tree. (default=None)\n",
    "- **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value. (default=0.)\n",
    "\n",
    "Other parameters that are not relevant to the training process are not listed here. From the hyperparameters, we can have a sense of how the algorithm is implemented. \n",
    "\n",
    "An unnecessary evil to use a classifier is tuning hypermeters to achieve the best performance. Since this tutorial does not focus on improving performance but the comparison of three similar algorithms, I will use the default setting and only change the number of trees to make it consistent among three algorithms. The criteria I use to compare are the training time and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time = 19.93053889274597\n",
      "Test accuracy = 0.833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=10, \n",
    "                                 learning_rate=0.1, \n",
    "                                 max_depth=3)\n",
    "\n",
    "# start training\n",
    "start_time = time.time()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))\n",
    "\n",
    "# prediction and evaluation \n",
    "pred = clf.predict(Xtest)\n",
    "accuracy = np.sum(pred == ytest) / pred.shape[0]\n",
    "print('Test accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice property of GBDT is that feature importance can be provided after training. It is evaluated by counting in how many trees a feature is split. In sklearn, we can extract the feature importance from the classifier using clf.feature\\_importances\\_ and it is given as the percentage rather than the count. Below, the histogram of feature importance is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0222245992562 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEBZJREFUeJzt3X+MpVV9x/H3p6xgQ9Xlx0Do7qaL\ncdNKmyh0QtfQNC1braBxSSoJjZEN2WTTlDY2NmnX2qZp0z/gn6LEhmYj2qXRAsWa3Si1kgVibAo6\nWxBFpAyUsuNSdiw/KhI1tN/+MWd02J3duTNz787M4f1KnjzPc57zPPfckzufe/bc595NVSFJ6tdP\nrHQDJEmjZdBLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOrdupRsAcPbZZ9fmzZtX\nuhmStKYcPHjwO1U1tlC9VRH0mzdvZmJiYqWbIUlrSpL/HKSeUzeS1DmDXpI6Z9BLUucMeknqnEEv\nSZ0z6CWpcwa9JHXOoJekzhn0ktS5VfHN2OXYvPvzK/bYT173rhV7bEkalCN6SeqcQS9JnTPoJalz\nBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzg0U9EnWJ7kjybeS\nPJLkbUnOTHJXksfa+oxWN0luTDKZ5KEkF432KUiSTmTQEf1HgS9U1c8BbwEeAXYDB6pqC3Cg7QNc\nBmxpyy7gpqG2WJK0KAsGfZLXA78C3AxQVT+squeB7cDeVm0vcEXb3g7cUjPuA9YnOW/oLZckDWSQ\nEf0bgWngk0keSPLxJKcD51bV0wBtfU6rvwE4NOf8qVYmSVoBgwT9OuAi4KaquhD4Hj+epplP5imr\nYyolu5JMJJmYnp4eqLGSpMUbJOingKmqur/t38FM8D8zOyXT1kfm1N805/yNwOGjL1pVe6pqvKrG\nx8bGltp+SdICFgz6qvov4FCSn21F24BvAvuBHa1sB7Cvbe8Hrm5332wFXpid4pEknXyD/ufgvwd8\nKsmpwBPANcy8SdyeZCfwFHBlq3sncDkwCbzU6kqSVshAQV9VDwLj8xzaNk/dAq5dZrskSUPiN2Ml\nqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6\nZ9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6txAQZ/kySRfT/Jg\nkolWdmaSu5I81tZntPIkuTHJZJKHklw0yicgSTqxxYzof62q3lpV421/N3CgqrYAB9o+wGXAlrbs\nAm4aVmMlSYu3nKmb7cDetr0XuGJO+S014z5gfZLzlvE4kqRlGDToC/hikoNJdrWyc6vqaYC2PqeV\nbwAOzTl3qpW9QpJdSSaSTExPTy+t9ZKkBa0bsN4lVXU4yTnAXUm+dYK6maesjimo2gPsARgfHz/m\nuCRpOAYa0VfV4bY+AnwWuBh4ZnZKpq2PtOpTwKY5p28EDg+rwZKkxVkw6JOcnuR1s9vAO4BvAPuB\nHa3aDmBf294PXN3uvtkKvDA7xSNJOvkGmbo5F/hsktn6n66qLyT5KnB7kp3AU8CVrf6dwOXAJPAS\ncM3QWy1JGtiCQV9VTwBvmaf8v4Ft85QXcO1QWidJWja/GStJnTPoJalzBr0kdc6gl6TOGfSS1DmD\nXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+gl\nqXMGvSR1zqCXpM4Z9JLUOYNekjo3cNAnOSXJA0k+1/bPT3J/kseS3Jbk1FZ+WtufbMc3j6bpkqRB\nLGZE/wHgkTn71wM3VNUW4DlgZyvfCTxXVW8Cbmj1JEkrZKCgT7IReBfw8bYf4FLgjlZlL3BF297e\n9mnHt7X6kqQVMOiI/iPAHwL/1/bPAp6vqpfb/hSwoW1vAA4BtOMvtPqSpBWwYNAneTdwpKoOzi2e\np2oNcGzudXclmUgyMT09PVBjJUmLN8iI/hLgPUmeBG5lZsrmI8D6JOtanY3A4bY9BWwCaMffADx7\n9EWrak9VjVfV+NjY2LKehCTp+BYM+qr6UFVtrKrNwFXA3VX1PuAe4L2t2g5gX9ve3/Zpx++uqmNG\n9JKkk2M599H/EfDBJJPMzMHf3MpvBs5q5R8Edi+viZKk5Vi3cJUfq6p7gXvb9hPAxfPU+T5w5RDa\nJkkaAr8ZK0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6g\nl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnFgz6\nJK9N8pUkX0vycJI/b+XnJ7k/yWNJbktyais/re1PtuObR/sUJEknMsiI/gfApVX1FuCtwDuTbAWu\nB26oqi3Ac8DOVn8n8FxVvQm4odWTJK2QBYO+ZrzYdl/TlgIuBe5o5XuBK9r29rZPO74tSYbWYknS\nogw0R5/klCQPAkeAu4DHgeer6uVWZQrY0LY3AIcA2vEXgLPmueauJBNJJqanp5f3LCRJxzVQ0FfV\n/1bVW4GNwMXAm+er1tbzjd7rmIKqPVU1XlXjY2Njg7ZXkrRIi7rrpqqeB+4FtgLrk6xrhzYCh9v2\nFLAJoB1/A/DsMBorSVq8Qe66GUuyvm3/JPDrwCPAPcB7W7UdwL62vb/t047fXVXHjOglSSfHuoWr\ncB6wN8kpzLwx3F5Vn0vyTeDWJH8JPADc3OrfDPxdkklmRvJXjaDdkqQBLRj0VfUQcOE85U8wM19/\ndPn3gSuH0jpJ0rL5zVhJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9J\nnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktS5\nBYM+yaYk9yR5JMnDST7Qys9McleSx9r6jFaeJDcmmUzyUJKLRv0kJEnHN8iI/mXgD6rqzcBW4Nok\nFwC7gQNVtQU40PYBLgO2tGUXcNPQWy1JGtiCQV9VT1fVv7Xt7wKPABuA7cDeVm0vcEXb3g7cUjPu\nA9YnOW/oLZckDWRRc/RJNgMXAvcD51bV0zDzZgCc06ptAA7NOW2qlUmSVsDAQZ/kp4DPAL9fVf9z\noqrzlNU819uVZCLJxPT09KDNkCQt0kBBn+Q1zIT8p6rqH1vxM7NTMm19pJVPAZvmnL4ROHz0Natq\nT1WNV9X42NjYUtsvSVrAIHfdBLgZeKSq/mrOof3Ajra9A9g3p/zqdvfNVuCF2SkeSdLJt26AOpcA\n7we+nuTBVvbHwHXA7Ul2Ak8BV7ZjdwKXA5PAS8A1Q22xJGlRFgz6qvoy88+7A2ybp34B1y6zXZKk\nIfGbsZLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknq\nnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1bsGgT/KJ\nJEeSfGNO2ZlJ7kryWFuf0cqT5MYkk0keSnLRKBsvSVrYICP6vwXeeVTZbuBAVW0BDrR9gMuALW3Z\nBdw0nGZKkpZqwaCvqi8Bzx5VvB3Y27b3AlfMKb+lZtwHrE9y3rAaK0lavKXO0Z9bVU8DtPU5rXwD\ncGhOvalWdowku5JMJJmYnp5eYjMkSQsZ9oexmaes5qtYVXuqaryqxsfGxobcDEnSrKUG/TOzUzJt\nfaSVTwGb5tTbCBxeevMkScu11KDfD+xo2zuAfXPKr25332wFXpid4pEkrYx1C1VI8vfArwJnJ5kC\n/gy4Drg9yU7gKeDKVv1O4HJgEngJuGYEbZYkLcKCQV9Vv3WcQ9vmqVvAtcttlCRpePxmrCR1zqCX\npM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1LkFf+tGx7d59+dX5HGf\nvO5dK/K4ktYmR/SS1DmDXpI6Z9BLUucMeknqnEEvSZ3zrps1aKXu9gHv+JHWIkf0ktQ5R/RaFL87\nIK09juglqXMGvSR1biRBn+SdSR5NMplk9ygeQ5I0mKHP0Sc5Bfhr4O3AFPDVJPur6pvDfiy9evjZ\ngLR0o/gw9mJgsqqeAEhyK7AdMOi15qzkrayvRr6xjsYogn4DcGjO/hTwSyN4HEmdeTW+sZ6MN7dR\nBH3mKatjKiW7gF1t98Ukjy7x8c4GvrPEc3tln7yS/XEs++SVVqw/cv2yTv+ZQSqNIuingE1z9jcC\nh4+uVFV7gD3LfbAkE1U1vtzr9MQ+eSX741j2ySv13h+juOvmq8CWJOcnORW4Ctg/gseRJA1g6CP6\nqno5ye8C/wycAnyiqh4e9uNIkgYzkp9AqKo7gTtHce15LHv6p0P2ySvZH8eyT16p6/5I1TGfk0qS\nOuJPIEhS51Zd0C/08wlJTktyWzt+f5LNc459qJU/muQ3Br3majai/ngyydeTPJhk4uQ8k+FZap8k\nOSvJPUleTPKxo875xdYnk0luTDLfbcKr0oj64952zQfbcs7JeTbDsYw+eXuSg+21cDDJpXPOWbOv\nEapq1SzMfHj7OPBG4FTga8AFR9X5HeBv2vZVwG1t+4JW/zTg/HadUwa55mpdRtEf7diTwNkr/fxW\noE9OB34Z+G3gY0ed8xXgbcx8D+SfgMtW+rmucH/cC4yv9PNbgT65EPjptv0LwLfX+mukqlbdiP5H\nP59QVT8EZn8+Ya7twN62fQewrb2zbgduraofVNV/AJPteoNcc7UaRX+sdUvuk6r6XlV9Gfj+3MpJ\nzgNeX1X/WjN/0bcAV4z0WQzP0PujA8vpkweqavZ7Pw8Dr22j/7X8Gll1QT/fzydsOF6dqnoZeAE4\n6wTnDnLN1WoU/QEz31T+Yvun6S7WluX0yYmuObXANVerUfTHrE+2aZs/XVPTFMPrk98EHqiqH7C2\nXyOr7n+YGuTnE45X53jl872ZrZVbjUbRHwCXVNXhNu96V5JvVdWXltHOk2k5fbKca65Wo+gPgPdV\n1beTvA74DPB+Zkaxa8Gy+yTJzwPXA+9YxDVXrdU2oh/k5xN+VCfJOuANwLMnOHegn2RYpUbRH8z+\n07SqjgCfZW1N6SynT050zY0LXHO1GkV/UFXfbuvvAp/mVfQaSbKRmb+Lq6vq8Tn11+prZNUF/SA/\nn7Af2NG23wvc3ebM9gNXtfm084EtzHx4spZ/kmHo/ZHk9DZKI8npzIxYvnESnsuwLKdP5lVVTwPf\nTbK1TVFcDewbftNHYuj9kWRdkrPb9muAd/MqeY0kWQ98HvhQVf3LbOU1/hpZXXfdtNfe5cC/M/Op\n+Ydb2V8A72nbrwX+gZkPF78CvHHOuR9u5z3KnE/E57vmWlmG3R/M3InwtbY8vNb6Ywh98iQzI7cX\nmRmlXdDKx5kJs8eBj9G+TLgWlmH3BzN34xwEHmqvkY/S7thaK8tS+wT4E+B7wINzlnPW+mvEb8ZK\nUudW29SNJGnIDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjr3//kMbmN9lhAaAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110b5bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(clf.feature_importances_)\n",
    "print(max(clf.feature_importances_), min(clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find that most of the features do not make any contributions to build the tree. Another thing data scientists usually do is to filter the features based on the feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEFCAYAAADjUZCuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wNmDvp0Z2Wz5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a28df0978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "d = {}\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.01:\n",
    "        d[i] = clf.feature_importances_[i]\n",
    "\n",
    "sorted_feature_importances = OrderedDict(sorted(d.items(), key=lambda x:x[1], reverse=True))\n",
    "D = sorted_feature_importances\n",
    "rects = plt.bar(range(len(D)), D.values(), align='center')\n",
    "plt.xticks(range(len(D)), D.keys(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization can help us better know what features are important and how important they are. The dataset we are using is images. So one feature corresponds to one pixel. In some general purpose application, like house price prediction, the features might be the size of the house, the location of the house, etc. With feature importance, we can know which feature is most relevant to house price. \n",
    "\n",
    "Feature selection based on feature importance might help to reduce the feature space and make training faster. But we are actually losing information. I would do feature selection only if I have too many features and many of them are useless. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost [2] provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. If you read this paper, you will find that the most significant part is that XGBoost formally defines an objective function using gradient statistics including first level gradient (Gradient) and second level gradient (Hessian). For input samples, we can calculate the corresponding G and H, push them to the leaves they belong to and sum them up to evaluate how good a tree is (as shown in the following figure). The nice thing about the objective function is that we can add regularization terms to it to control model complexity, like adding L1 or L2 penalty to linear models. XGBoost adds the penalty to the number of leaves and scores on leaves in the objective function.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/struct_score.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Besides accuracy, fast is also a significant feature of XGBoost. The underlying implementation is C and it makes full use of available computing resources and supports parallel processing. Living in a world of big data, data become more and more, it is very inefficient to train with low speed. That is why XGBoost is so popular in Kaggle competition. \n",
    "\n",
    "Following [this guide](https://xgboost.readthedocs.io/en/latest/build.html#), you would be able to install xgboost as a python package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time = 7.43514609336853\n",
      "Test accuracy = 0.839\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# read data into Xgboost DMatrix format\n",
    "dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n",
    "dtest = xgb.DMatrix(Xtest, label=ytest)\n",
    "\n",
    "# specify parameters via map\n",
    "params = {\n",
    "    'booster':'gbtree',     #  tree-based models\n",
    "    'objective': 'multi:softmax', \n",
    "    'num_class':10, \n",
    "    'eta': 0.1,             # Same to learning rate\n",
    "    'gamma':0,              # Similar to min_impurity_decrease in GBDT\n",
    "    'alpha': 0,             # L1 regularization term on weight (analogous to Lasso regression)\n",
    "    'lambda': 2,            # L2 regularization term on weights (analogous to Ridge regression)\n",
    "    'max_depth': 3,         # Same as the max_depth of GBDT\n",
    "    'subsample': 1,         # Same as the subsample of GBDT\n",
    "    'colsample_bytree': 1,  # Similar to max_features in GBM\n",
    "    'min_child_weight': 1,  # minimum sum of instance weight (Hessian) needed in a child\n",
    "    'nthread':1,            # default to maximum number of threads available if not set\n",
    "}\n",
    "num_round = 10\n",
    "\n",
    "# start training\n",
    "start_time = time.time()\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))\n",
    "\n",
    "# get prediction and evaluate\n",
    "ypred = bst.predict(dtest)\n",
    "accuracy = np.sum(ypred == ytest) / ypred.shape[0]\n",
    "print('Test accuracy = {}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above you can find that xgboost has more hyperparameters than GBDT, like alpha, lambda, and min_child_weight due to the additional implementations. Alpha and lambda are used to control regularization, and min_child_weight is another criteria to control node split as a result of using second level gradient Hessian. Wonder about other parameters? Check [this](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM [3] is open sourced by Microsoft. It further optimizes xgboost's executing efficiency by reducing calculation cost of split gain, using histogram subtraction for speed-up, reducing memory usage, and reducing communication cost for parallel learning. Check out the paper, it is really impressive.   \n",
    "\n",
    "The most significant change for increasing accuracy is using leaf-wise tree growth instead of level-wise tree growth. GBDT and XGBoost have constraints on the max depth of the tree, while LightGBM set it None as default. From the following two figures, you can find the difference. \n",
    "<img src=\"https://lightgbm.readthedocs.io/en/latest/_images/level-wise.png\" width=\"400\" height=\"400\" />\n",
    "<img src=\"https://lightgbm.readthedocs.io/en/latest/_images/leaf-wise.png\" width=\"500\" height=\"500\" />\n",
    "Leaf-wise tree growth only split the node with the maximum split gain using a priority queue, while level-wise tree growth split every leaf node if it is splittable. Leaf-wise learning can fit data better and eliminate unhelpful split. \n",
    "\n",
    "\n",
    "Follow [this guide](https://github.com/Microsoft/LightGBM/tree/master/python-package) to install lightGBM as a python package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time = 5.919008016586304\n",
      "Test accuracy = 0.892\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(Xtrain, label=ytrain)\n",
    "test_data = lgb.Dataset(Xtest, label=ytest)\n",
    "\n",
    "# specify parameters via map\n",
    "params = {\n",
    "    'num_leaves':31,                # Same to max_leaf_nodes in GBDT, but GBDT's default value is None\n",
    "    'max_depth': -1,                # Same to max_depth of xgboost\n",
    "    'tree_learner': 'serial', \n",
    "    'application':'multiclass',     # Same to objective of xgboost\n",
    "    'num_class':10,                 # Same to num_class of xgboost\n",
    "    'learning_rate': 0.1,           # Same to eta of xgboost\n",
    "    'min_split_gain': 0,            # Same to gamma of xgboost\n",
    "    'lambda_l1': 0,                 # Same to alpha of xgboost\n",
    "    'lambda_l2': 0,                 # Same to lambda of xgboost\n",
    "    'min_data_in_leaf': 20,         # Same to min_samples_leaf of GBDT\n",
    "    'bagging_fraction': 1.0,        # Same to subsample of xgboost\n",
    "    'bagging_freq': 0,\n",
    "    'bagging_seed': 0,\n",
    "    'feature_fraction': 1.0,         # Same to colsample_bytree of xgboost\n",
    "    'feature_fraction_seed': 2,\n",
    "    'min_sum_hessian_in_leaf': 1e-3, # Same to min_child_weight of xgboost\n",
    "    'num_threads': 1\n",
    "}\n",
    "num_round = 10\n",
    "\n",
    "# start training\n",
    "start_time = time.time()\n",
    "bst = lgb.train(params, train_data, num_round)\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))\n",
    "\n",
    "# get prediction and evaluate\n",
    "ypred_onehot = bst.predict(Xtest)\n",
    "ypred = []\n",
    "for i in range(len(ypred_onehot)):\n",
    "    ypred.append(ypred_onehot[i].argmax())\n",
    "\n",
    "accuracy = np.sum(ypred == ytest) / len(ypred)\n",
    "print('Test accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to leaf-wise tree growth, the primary key to control the tree structure is num_leaves rather than max_depth. Leaf-wise tree growth can achieve better accuracy but easily overfit the data. To address this problem, LightGBM implements many tricks, like dropout, control bagging frequency, etc. There are much more parameters than listed above to help addressing overfitting. Check [this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst) for all parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison and Conclusion\n",
    "\n",
    "With the same data, number of trees and the number of thread, the training time and accuracy of three algorithms are shown in the table. \n",
    "\n",
    "|          | time(s) | accuracy(%) |\n",
    "|----------|---------|-------------|\n",
    "| GBDT     | 19.93   | 0.833       |\n",
    "| XGBoost  | 7.43    | 0.839       |\n",
    "| LightGBM | 5.92    | 0.892       |\n",
    "\n",
    "In terms of training time, xgboost significantly reduces the training time even thought only one thread is used. LightGBM further shortens the time.  \n",
    "\n",
    "In terms of accuracy, current experiments are not sufficient to make any conclusion as the hyperparameters are not tuned to achieve the best performance. But we can still have some analysis of the results. It is reasonable that xgboost behavior better than GBDT as it involves second level gradient Hessian. And xgboost should perform better in the long-term training as it applies regularization techniques. There is no doubt that LightGBM performs best in the first 10 runs as a result of leaf-wise tree growth. \n",
    "\n",
    "This tutorial provides brief information about three algorithms. Want to explore more about the underlying theory and system architecture? Check the paper and source code in Reference. \n",
    "\n",
    "Happy training! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Friedman, Jerome H. \"[Greedy function approximation: a gradient boosting machine.](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)\" Annals of statistics (2001): 1189-1232.   \n",
    "[2] Chen, Tianqi, and Carlos Guestrin. \"[Xgboost: A scalable tree boosting system.](https://arxiv.org/pdf/1603.02754.pdf)\" In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785-794. ACM, 2016.   \n",
    "[3] Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. \"[LightGBM: A highly efficient gradient boosting decision tree.](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)\" In Advances in Neural Information Processing Systems, pp. 3149-3157. 2017.   \n",
    "[4] GBDT source code:  https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/gradient_boosting.py#L1189   \n",
    "[5] XGBoost source code: https://github.com/dmlc/xgboost    \n",
    "[6] LightGBM source code: https://github.com/Microsoft/LightGBM  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
